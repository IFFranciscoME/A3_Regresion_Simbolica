
# -- --------------------------------------------------------------------------------------------------- -- #
# -- project: Multivariate Linear Regression with Symbolic Regressors and L1L2 Regularization            -- #
# -- for future prices prediction, the case for UsdMxn                                                   -- #
# -- script: notes.txt : text file with general notes                                                    -- #
# -- author: IFFranciscoME                                                                               -- #
# -- license: GPL-3.0 License                                                                            -- #
# -- repository: https://github.com/IFFranciscoME/A3_Regresion_Simbolica                                 -- #
# -- --------------------------------------------------------------------------------------------------- -- #

# este es un ejemplo de como declarar cada variable como simbolica
# sustituir una variable simbolica en la expresion
# tener un objeto tipo simbolico sympy

# evaluar la variable con el valor que se desea para agregar resultado numerico de la
# expresion simbolica en el cuadro de features
# data_features.eval("gplearn={}".format(exp_sim), inplace=True)

# PROCESO: buscar el menor score en la funcion simbolic regresor modificando los procentajes de
# mutación, de stopping criteria, y la semilla: encontrado fue aprox .305
# dividir para obtener las variables (4 se me hizo el numero óptimo al ver exp_sim)
# se logró reducir hasta 62,134 el rss
# observación: "abs" y "sen" no estan en op_sim, no se están tomando en cuenta por lo que veo

# -- PASOS a seguir
# 1ero - Regresion simbolica para obtener regresores "Individuales", Hacer esto manualmente.
# (Ingenieria de variables simbolicas a traves un proceso de separacion de componentes de una regresion
# simbolica con gplearn).
# Una prueba semi/manual en python con un modelo simple, ridge, lasso y elasticnet.
# revisar como separar las subcomponentes del resultado de la regreion simbolica sistematicamente.
# ma_hl_5*(lag_ol_5 + ma_ho_6 - ma_oi_6)*(lag_ol_6 + 0.074 + ma_hl_3*(lag_ol_4 - ma_ol_3)/lag_ho_4)
# paralelizar proces de uso de gplearn para generar unos regresores muy buenos y correlacionados con
# la variable a explicar.
# 2do - Un muy buen proceso de ajuste de un modelo de Regresion lineal.
# 3ero - Un muy buen proceso de ajuste de un modelo de Regresion.

# -- Elementos de automatizacion de extraccion de features
# separar el feature general en N sub-ecuaciones

# evaluar cada una de las N sub-equaciones para generar N nuevos features
# -- Bajo el supuesto de haber separado los n = 4 sub ecuaciones

# Otro experimento es hacer el producto hadamard con features en el mismo punto del tiempo (periodo)
# ej: lag_hl_1 (hadamard) ma_hl_1

# -- jUEVES 29.10
# IDEA A FUTURO: UN BOSQUE DE ARBOLES Y CADA ARBOL ES UN CONJUNTO DE OPERACIONES SIMBOLICAS A LOS FEATURES
DE ENTRADA.

- Hacer varios casos para comparar
- Primer paso: tomar los features originales, introducirlos en regresion simbolica, observar las operaciones
que resultan de la regresion simbolica, extraer a mano las operaciones, hacer las operaciones con los
features originales, correr regresion lineal clasica con los features originales y con los features nuevos
(transformaciones resultantes de las operaciones utilizadas en regresion simbolica).

RUTAS HEURISTICAS V0.1

"ramped half and half"

- Profundidad inicial, con el termino init_depth, para la poblacion inicial (arbol inicial) que sea en un
  rango muy variado y amplio para poder tener arboles amplios de donde se elija una gran variedad de
  sub-arboles y de nodos.
- Metodo de inicializacion, con el termino init_method, con un valor de "Half and Half", que es
  tener la posibilidad de generar arboles con variaciones entre sus parametros de profundidad y amplitud,
  expresadas en "depth" y "grow"
- Con ambas cosas anteriores se esta inicializando con el metodo "ramped half and half" como se menciona
  en el documento de apoyo (paper).

"Objetivo por correlacion"

- la metrica de ajuste, con el parametro "metric", se elige "pearson". Para poder generar varias
  variables explicativas a la vez, se maximiza la correlacion de estas respecto a la variable a explicar, y
  se hace el calculo con el valor absoluto para que tambien se incluyan variables explicativas con
  correlacion alta negativa.
  Otra sugerencia es que se utilice pearson cuando las variables seran utilizadas para un modelo lineal,
  y spearman para cuando es un modelo basado en arboles. Para el caso de un modelo lineal, hace sentido
  puesto que las variables explicativas son relacionadas mediante una combinacion lineal de las mismas
  para realizar la prediccion.
  Los resultados de las variables generadas con menor correlacion entre ellas se reportan en el hall_of_fame,
  y con el n_components se seleccionan las top N variables de acuerdo a su correlacion con la variable
  explicativa.

"Cross over y mutaciones".

- p_crossover: consiste en dos torneos, en el primero se elige al ganador (menor error) y aleatoriamente se elige
ua rama para cambiar (se llama padre). Se hace un segundo torneo, y del ganador se elige una rama aleatoriamente
(se llama donador). entonces se cambiar el padre con una rama del donador para generar un nuevo arbol.
Entre mayor sea el parámetro, mayor la probabilidad de que se haga este proceso en la creación de un arbol
- p_subtree_mutation. Parecido al p_crossover, excepto que en este caso, cuando se hace, el donador es
generado aleatoriamente sin necesidad de un torneo para ser donador (más agresivo).
- p_hoist_mutation: se usa para quitar elementos del arbol, se selecciona una rama, y se elige uno de
los elementos de esta. Se cambia toda la rama por solo ese elemento
- p_point_mutation: del padre se seleccionan algunos nodos y se cambian. Si es una variable se cambia por una variable
si es una operación se cambia por una operación. Para saber cuantos cambios se hacen, se usa p_point_replace,
donde se establece el promedio de cambios (donde 0 sería ningun cambio y 1 cambio en todos los nodos)


"Parsimonia del proceso".

- Pendiente.

# ---------------------------------------------------------------------------------------------- CHARLA -- #

Haremos una charla de divulgacion sobre el tema central de este trabajo. Será abierto a la comunidad ITESO,
con una duración de 2 horas, se dará un certificado de asistencia a las personas inscritas. El propósito
principal de esta charla es comunicar, de una manera amigable y con muchos ejemplos, el tema del uso de
gplearn para generar variables explicativas no lineales para usarlas en modelos lineales. Se va a compartir
un repositorio especial en github para que estén disponibles los códigos de ejemplo y notebooks. Será en
línea.

Pendientes iniciales
- NOMBRE DE CHARLA
- PEQUEÑA DESCRIPCION
- DIA Y HORA
- PLATAFORMA DE CONEXIÓN
- REPOSITORIO
- PONENTES: Francisco y Diana
- PRESENTADOR: Riemann

Pendientes posteriores
- INVITACIÓN TIPO POSTER CON NOMBRE, PEQUEÑA DESCRIPCIÓN, DIA, HORA, PLATAFORMA, PONENTES
- SESIÓN DE PRUEBA

# --------------------------------------------------------------------------------- PENDIENTES 6.NOV -- #

- Funcion para crear data set con futuros continuos (tener datos diarios).
- - Esperaremos a resultados de PAP
- Rutas heurísticas para definición de parámetros en gplearn.
- - Mutaciones y crossover (Diana)
- - Parsimonia del proceso (Francisco)
- Decidir y hacer divisiones de datos para train, test. (Diana)
- Ajuste de modelos (Francisco)
- - OLS
- - OLS con L1L2
- - LS-SVM
- Medidas de desempeño (Francisco).
- Comparacion entre modelos (Diana).
- Análisis de resultados de ajuste (Diana).
- Visualización de resultados (Francisco - Diana).
- Conclusiones (Francisco - Diana).
- Escribir secciones para el articulo (Pendiente).
