
# -- --------------------------------------------------------------------------------------------------- -- #
# -- project: Multivariate Linear Regression with Symbolic Regressors and L1L2 Regularization            -- #
# -- for future prices prediction, the case for UsdMxn                                                   -- #
# -- script: notes.txt : text file with general notes                                                    -- #
# -- author: IFFranciscoME                                                                               -- #
# -- license: GPL-3.0 License                                                                            -- #
# -- repository: https://github.com/IFFranciscoME/A3_Regresion_Simbolica                                 -- #
# -- --------------------------------------------------------------------------------------------------- -- #



# este es un ejemplo de como declarar cada variable como simbolica
# sustituir una variable simbolica en la expresion
# tener un objeto tipo simbolico sympy

# evaluar la variable con el valor que se desea para agregar resultado numerico de la
# expresion simbolica en el cuadro de features
# data_features.eval("gplearn={}".format(exp_sim), inplace=True)

# PROCESO: buscar el menor score en la funcion simbolic regresor modificando los procentajes de
# mutación, de stopping criteria, y la semilla: encontrado fue aprox .305
# dividir para obtener las variables (4 se me hizo el numero óptimo al ver exp_sim)
# se logró reducir hasta 62,134 el rss
# observación: "abs" y "sen" no estan en op_sim, no se están tomando en cuenta por lo que veo

# -- PASOS a seguir
# 1ero - Regresion simbolica para obtener regresores "Individuales", Hacer esto manualmente.
# (Ingenieria de variables simbolicas a traves un proceso de separacion de componentes de una regresion
# simbolica con gplearn).
# Una prueba semi/manual en python con un modelo simple, ridge, lasso y elasticnet.
# revisar como separar las subcomponentes del resultado de la regreion simbolica sistematicamente.
# ma_hl_5*(lag_ol_5 + ma_ho_6 - ma_oi_6)*(lag_ol_6 + 0.074 + ma_hl_3*(lag_ol_4 - ma_ol_3)/lag_ho_4)
# paralelizar proces de uso de gplearn para generar unos regresores muy buenos y correlacionados con
# la variable a explicar.
# 2do - Un muy buen proceso de ajuste de un modelo de Regresion lineal.
# 3ero - Un muy buen proceso de ajuste de un modelo de Regresion.

# -- Elementos de automatizacion de extraccion de features
# separar el feature general en N sub-ecuaciones

# evaluar cada una de las N sub-equaciones para generar N nuevos features
# -- Bajo el supuesto de haber separado los n = 4 sub ecuaciones

# Otro experimento es hacer el producto hadamard con features en el mismo punto del tiempo (periodo)
# ej: lag_hl_1 (hadamard) ma_hl_1
